{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPLM0EjcDFhES3Jqyewg9HW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeonjin99/TIL/blob/main/Search_using_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. LangChain을 사용하여 문장 임베딩, 벡터 스토어, 문서 검색을 설정하기\n",
        "- 텍스트 데이터에서 임베딩을 생성하고, 임베딩을 바탕으로 특정 질의에 맞는 문서를 검색하는 것"
      ],
      "metadata": {
        "id": "4czHR4bxsv3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYB-SdbQqcBy"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai docarray tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere"
      ],
      "metadata": {
        "id": "gqDwURonqmgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules required\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
      ],
      "metadata": {
        "id": "t541wI2JqwBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 임베딩 및 벡터 스토어 생성\n",
        "- 아래 문장들에 대해 **임베딩(Embedding)** 생성 -> 벡터 스토어에 저장\n",
        "- 벡터 스토어는 문장을 벡터 형식으로 저장한 다음, 사용자가 입력한 질의에 따라 유사도를 기반으로 가장 유사한 문장을 검색"
      ],
      "metadata": {
        "id": "obGWIE0JtVIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 및 벡터 스토어 생성\n",
        "vectorstore = DocArrayInMemorySearch.from_texts(\n",
        "    [\"There are 4 seasons in Korea\",\n",
        "     \"Harry Potter is an outstanding wizard\",\n",
        "     \"The dog likes to walk\"],\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")"
      ],
      "metadata": {
        "id": "BQQ2JUaPr9y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 검색기 세팅\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "AydUcuTPsm79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 검색기 실행\n",
        "retriever.invoke(\"Who is Harry Potter?\")"
      ],
      "metadata": {
        "id": "8FgQPSY-tII7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. LangChain을 사용한 벡터 기반 검색 & OpenAI 모델을 통한 질의 응답 생성"
      ],
      "metadata": {
        "id": "NoEutU_quqdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken"
      ],
      "metadata": {
        "id": "HJuYVP7zx2YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "pVVeR_gwx4Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 단순한 영어 질문에 대한 답변"
      ],
      "metadata": {
        "id": "cBtr_8oSu8xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터 스토어 생성 및 검색기 생성\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# 프롬프트 템플릿 생성\n",
        "#   -> 사용자가 질문에 대해 답변할 때 필요한 컨텍스트와 질문을 받아, OpenAI 모델에게 전달할 포맷을 구성\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "EEOR40XCx9Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 및 체인 설정\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# 체인은 여러 단계로 구성된 파이프라인\n",
        "#   -> context: retriever에서 검색된 문장을 전달\n",
        "#   -> question: 사용자가 입력한 질문을 전달(RunnablePassthrough는 변환 없이 전달)\n",
        "#   -> StrOutputParser: 모델의 출력을 문자열로 파싱\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "lxzaCUtkx_Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체인 실행\n",
        "chain.invoke(\"where did harrison work?\")"
      ],
      "metadata": {
        "id": "tdBRoRtKyO1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 여러 언어 중 하나로 답변"
      ],
      "metadata": {
        "id": "dV0tSUqXvA7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"language\": itemgetter(\"language\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "tXrxrI8yyQZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다국어 체인 실행\n",
        "chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})"
      ],
      "metadata": {
        "id": "VOXqMnDmyT49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}